# Optimizers

Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. At the same time, every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize gradient descent.

For more information read [this blog post](http://ruder.io/optimizing-gradient-descent/).

## Stochastic gradient descent (SGD)

![Stochastic gradient descent](http://hduongtrong.github.io/assets/gradient_descent/all.gif)